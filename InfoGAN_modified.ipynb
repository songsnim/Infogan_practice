{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오늘 날짜 : 2022-01-17\n",
      "GPU device : cuda:2\n",
      "image.size() =  torch.Size([128, 1, 32, 32]) \ttype torch.FloatTensor\n",
      "label.size() =  torch.Size([128]) \ttype torch.LongTensor\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x72 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x72 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import math\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "args = {\n",
    "        'GPU_NUM' : 2,\n",
    "        'Epochs' : 200,\n",
    "        'batch_size' : 128,\n",
    "        'lr' : 0.0002,\n",
    "        'b1' : 0.5,\n",
    "        'b2' : 0.999,\n",
    "        'latent_dim' : 62,\n",
    "        'code_dim' : 2,\n",
    "        'n_classes' : 2,\n",
    "        'img_size' : 32,\n",
    "        'channels' : 1,\n",
    "        'sample_interval' : 400\n",
    "        }\n",
    "\n",
    "device = torch.device('cuda:{}'.format(args['GPU_NUM']) if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(device) # change allocation of current GPU\n",
    "today = datetime.date.today()\n",
    "print('오늘 날짜 :',today)\n",
    "print('GPU device :', device)\n",
    "\n",
    "my_transform =transforms.Compose([transforms.Grayscale(num_output_channels=1),\n",
    "                                transforms.Resize(args['img_size']), \n",
    "                                transforms.ToTensor(), \n",
    "                                transforms.Normalize([0.5], [0.5])])\n",
    "\n",
    "train_data = ImageFolder('data/MNIST/classes/binary/train', transform = my_transform)\n",
    "test_data = ImageFolder('data/MNIST/classes/binary/test', transform = my_transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=args['batch_size'], shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=args['batch_size'], shuffle=True)\n",
    "\n",
    "pltsize = 1\n",
    "plt.figure(figsize=(10*pltsize,pltsize))\n",
    "for image, label in train_loader:\n",
    "    print('image.size() = ',image.size(), '\\ttype', image.type())\n",
    "    print('label.size() = ', label.size(), '\\ttype', label.type())\n",
    "    break\n",
    "\n",
    "plt.figure(figsize=(10*pltsize,pltsize))\n",
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image[i,:,:,:].permute(1,2,0), cmap=\"gray\")\n",
    "    plt.title('class '+ str(label[i].item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_np(tensor):\n",
    "    return tensor.cpu().detach().numpy()\n",
    "\n",
    "def weights_init_normal(m):\n",
    "    \"\"\" Conv layer는 mean이 0, std가 0.02인 가우시안 분포로 weight init\n",
    "        BatchNorm은 mean이 1, std가 0.02인 가우시안 분포로 weight init\n",
    "        Bias term은 전부 0으로 초기화\n",
    "    Args:\n",
    "        m ([model]): 학습하려는 모델\n",
    "    \"\"\"\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "def to_discrete(y, num_columns):\n",
    "    \"\"\" onehot encoding\n",
    "        (batch_size,)가 shape인 label이 있으면, (64,num_columns)인 zeros 행렬을 생성하고,\n",
    "        (batch_size,)의 label vector element 값의 index만 1로 바꿔서 one-hot encoding함\n",
    "    Args:\n",
    "        y : 어떤 array (y.shape[0]는 batch_size로 보면 됨)\n",
    "        num_columns : num_classes\n",
    "    \"\"\"\n",
    "    y_disc = np.zeros((y.shape[0], num_columns))\n",
    "    y_disc[range(y.shape[0]), y] = 1.0 # one-hot encoding()\n",
    "\n",
    "    return Variable(FloatTensor(y_disc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        #nn.Conv2d(input channel, output channel, ...)\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes,\n",
    "                               kernel_size = 3,\n",
    "                               stride=stride,\n",
    "                               padding=1,\n",
    "                               bias = False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes) # Batchnorm은 사이의 가중치가 아니라 출력 층만 노말라이징\n",
    "        self.conv2 = nn.Conv2d(planes, planes,\n",
    "                               kernel_size = 3,\n",
    "                               stride=1,\n",
    "                               padding=1,\n",
    "                               bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes,\n",
    "                          kernel_size = 1,\n",
    "                          stride=stride,\n",
    "                          bias=False),\n",
    "                nn.BatchNorm2d(planes))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class ResNet_3232(nn.Module):\n",
    "    def __init__(self, channels = 1, num_classes = 10):\n",
    "        super(ResNet_3232, self).__init__()\n",
    "        \n",
    "        self.rgb = channels\n",
    "        self.in_planes = 16\n",
    "        # RGB여서 3, in_planes는 내맘대로 16\n",
    "        self.conv1 = nn.Conv2d(self.rgb,16,\n",
    "                               kernel_size = 3,\n",
    "                               stride=1,\n",
    "                               padding=1,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(16, 2, stride=2)\n",
    "        self.layer2 = self._make_layer(32, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(64, 2, stride=2)\n",
    "        self.linear = nn.Linear(64,2)\n",
    "        \n",
    "    def _make_layer(self, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] *(num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(ResidualBlock(self.in_planes,planes,stride))\n",
    "            self.in_planes = planes\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        out1 = F.relu(self.bn1(self.conv1(x)))\n",
    "        out2 = self.layer1(out1)\n",
    "        out3 = self.layer2(out2)\n",
    "        out4 = self.layer3(out3)\n",
    "        out5 = F.avg_pool2d(out4, 4)\n",
    "        out6 = out5.view(out5.size(0), -1)\n",
    "        out7 = self.linear(out6)\n",
    "        \n",
    "        return out7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, log_interval):\n",
    "    model.train()\n",
    "    for batch_idx, (image, label) in enumerate(train_loader):\n",
    "        image = image.to(device)\n",
    "        label = label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(image)\n",
    "\n",
    "\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % log_interval ==0:\n",
    "            print(\"Train Epoch: {} [{}/{}({:.0f}%)]\\tTrain Loss: {:.6f}\".format(\n",
    "                Epoch, batch_idx*len(image), len(train_loader.dataset), 100.*batch_idx/len(train_loader), loss.item()))\n",
    "    return output\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss=0\n",
    "    correct=0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for image, label in test_loader:\n",
    "            image = image.to(device)\n",
    "            label = label.to(device)\n",
    "            output = model(image)\n",
    "            test_loss += criterion(output, label).item()\n",
    "            prediction = output.max(1,keepdim=True)[1] # output에서 제일 큰 놈의 index를 반환한다(이경우에 0 or 1)\n",
    "            correct += prediction.eq(label.view_as(prediction)).sum().item()\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100*correct / len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ResNet_3232(channels = args['channels'], num_classes=args['n_classes']).to(device)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# best_accuracy = 0\n",
    "# for Epoch in range(1, 10+1):\n",
    "#     train(model, train_loader, optimizer, log_interval=200)\n",
    "#     test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "#     if test_accuracy > best_accuracy:\n",
    "#         best_accuracy = test_accuracy\n",
    "#         torch.save(model, 'pretrained_model/ResNet_3232_1_7.pt')\n",
    "#         torch.save(model.state_dict(), 'pretrained_model/ResNet_3232_parameters_1_7.pt')\n",
    "#     print(\"[EPOCH: {}], \\tTest Loss: {:.4f},\\tTest Accuracy: {:.2f}%\\n\".format(\n",
    "#         Epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded output shape is (64, 4, 4)\n",
    "# continuous dim = 1 (가장 중요한 code 하나만 뽑아보자)\n",
    "# discrete dim = 2 (n_classes가 2)\n",
    "\n",
    "args['cont_dim_P'] = 1\n",
    "args['cont_dim_G'] = 2\n",
    "args['disc_dim'] = 2\n",
    "args['latent_dim'] = 32\n",
    "args['reduced_dim'] = args['cont_dim_P'] + args['cont_dim_G'] + args['disc_dim'] + args['latent_dim'] # 37\n",
    "\n",
    "class Predictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Predictor, self).__init__()\n",
    "            \n",
    "        self.input_dim = args['cont_dim_P']\n",
    "        self.predictor = nn.Sequential(nn.Linear(self.input_dim, 128),nn.ReLU(), nn.Dropout(0.2),\n",
    "                                        nn.Linear(128, 64), nn.ReLU(),nn.Dropout(0.2),\n",
    "                                        nn.Linear(64,2)) # predict class (1 or 7)\n",
    "        self.reduce_layer = nn.Linear(64*4*4, args['reduced_dim']) # 37 (total)\n",
    "        self.cont_layer_P = nn.Linear(args['reduced_dim'], args['cont_dim_P']) \n",
    "        self.cont_layer_G = nn.Linear(args['reduced_dim'], args['cont_dim_G'])\n",
    "        self.disc_layer = nn.Linear(args['reduced_dim'], args['disc_dim'], nn.Softmax())\n",
    "        self.latent_layer = nn.Linear(args['reduced_dim'], args['latent_dim'])\n",
    "        \n",
    "    def reduce_and_split_encoded(self, encoded):\n",
    "        encoded = encoded.view(encoded.shape[0], -1)\n",
    "        reduced = self.reduce_layer(encoded)\n",
    "        cont_code_P = self.cont_layer_P(reduced)\n",
    "        cont_code_G = self.cont_layer_G(reduced)\n",
    "        disc_code =  self.disc_layer(reduced)\n",
    "        latent = self.latent_layer(reduced)\n",
    "        return cont_code_P, cont_code_G, disc_code, latent\n",
    "            \n",
    "    def predict(self, cont_code_P):\n",
    "        predicted = self.predictor(cont_code_P)\n",
    "        return predicted\n",
    "        \n",
    "    def forward(self, encoded):\n",
    "        cont_code_P, cont_code_G, disc_code, latent = self.reduce_and_split_encoded(encoded)\n",
    "        predicted = self.predict(cont_code_P)\n",
    "        return predicted, cont_code_P, cont_code_G, disc_code, latent\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        input_dim = args['reduced_dim'] # 32 + 2 + 2 + 1\n",
    "        \n",
    "        self.init_size = args['img_size'] // 4\n",
    "        # conv에 넣을 수 있도록 dim_adjust\n",
    "        self.fc = nn.Sequential(nn.Linear(input_dim, 128 * self.init_size * self.init_size ))\n",
    "        \n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(128), nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128,128,3, stride=1, padding=1), nn.BatchNorm2d(128,0.8), nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128,64,3, stride=1, padding=1), nn.BatchNorm2d(64,0.8), nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(64,args['channels'], 3, stride=1, padding=1), \n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, cont_code_P, cont_code_G, disc_code, latent):\n",
    "        gen_input = torch.cat((cont_code_P, cont_code_G, disc_code, latent), dim=-1) # cat => 32+2+2+1 = 37 \\\n",
    "        # print(gen_input.shape, args['reduced_dim'])\n",
    "        #  mat1 and mat2 shapes cannot be multiplied (128x37 and 36x8192)\n",
    "        out = self.fc(gen_input)    \n",
    "        out = out.view(out.shape[0], 128, self.init_size, self.init_size) # block화\n",
    "        img = self.conv_blocks(out)\n",
    "        \n",
    "        return img\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        def discriminator_block(in_channels, out_channels, bn=True):\n",
    "            block = [nn.Conv2d(in_channels, out_channels, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n",
    "            if bn:\n",
    "                block.append(nn.BatchNorm2d(out_channels, 0.8))\n",
    "            return block\n",
    "\n",
    "        # batchnorm을 넣을지 말지에 따라서 discriminator_block의 모듈 수가 달라진다\n",
    "        # 달라지면 nn.Sequential에 추가할 때 if로 나눠서 해야하나? 싶지만\n",
    "        # *를 사용하면 block안에 모듈이 몇개든 그냥 싹다 넣어주는 역할을 한다.\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            *discriminator_block(args['channels'], 16, bn=False),\n",
    "            *discriminator_block(16, 32),\n",
    "            *discriminator_block(32, 64),\n",
    "            *discriminator_block(64, 128)\n",
    "        )\n",
    "        \n",
    "        downsample_size = args['img_size'] // (2**4) #stride 2인 block이 4개 있었으니까 4번 downsampled \n",
    "        \n",
    "        self.adv_layer = nn.Sequential(nn.Linear(128*downsample_size*downsample_size, 1)) # real or fake 예측 \n",
    "        self.disc_layer = nn.Sequential(nn.Linear(128*downsample_size*downsample_size, args['disc_dim']),\n",
    "                                       ) # class 예측 \n",
    "        self.cont_layer = nn.Sequential(nn.Linear(128*downsample_size*downsample_size, \n",
    "                                                    args['cont_dim_P']+args['cont_dim_G'])) # cont_code 예측\n",
    "        \n",
    "    def forward(self, img):\n",
    "        out = self.conv_blocks(img)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        reality = self.adv_layer(out)\n",
    "        disc_pred = self.disc_layer(out)\n",
    "        cont_pred = self.cont_layer(out)\n",
    "        \n",
    "        return reality, disc_pred, cont_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_loss = nn.CrossEntropyLoss().to(device)\n",
    "recon_loss = nn.MSELoss().to(device)\n",
    "adversarial_loss = nn.MSELoss().to(device)\n",
    "discrete_loss = nn.CrossEntropyLoss().to(device)\n",
    "continuous_loss = nn.MSELoss().to(device)\n",
    "\n",
    "lambda_disc = 1\n",
    "lambda_cont = 0.1 \n",
    "\n",
    "pretrained_resnet = ResNet_3232(channels=1, num_classes=2).to(device)\n",
    "pretrained_resnet.load_state_dict(torch.load('pretrained_model/ResNet_3232_parameters_1_7.pt'))\n",
    "\n",
    "E = nn.Sequential(*(list(pretrained_resnet.children())[:5])).to(device)\n",
    "P = Predictor().to(device)\n",
    "G = Generator().to(device)\n",
    "D = Discriminator().to(device)\n",
    "\n",
    "\n",
    "P.apply(weights_init_normal)\n",
    "G.apply(weights_init_normal)\n",
    "D.apply(weights_init_normal)\n",
    "\n",
    "# os.makedirs(\"../../data/mnist\", exist_ok=True)\n",
    "# dataloader = torch.utils.data.DataLoader(\n",
    "#     datasets.MNIST(\n",
    "#         root=\"../../data/mnist\",\n",
    "#         train=True,\n",
    "#         download=True,\n",
    "#         transform=transforms.Compose(\n",
    "#             [transforms.Resize(args['img_size']), \n",
    "#              transforms.ToTensor(), \n",
    "#              transforms.Normalize([0.5], [0.5])]\n",
    "#         ),\n",
    "#     ),\n",
    "#     batch_size=args['batch_size'],\n",
    "#     shuffle=True,\n",
    "# )\n",
    "\n",
    "optimizer_P = torch.optim.Adam(P.parameters(), lr=args['lr'], betas=(args['b1'], args['b2']))\n",
    "optimizer_G = torch.optim.Adam(G.parameters(), lr=args['lr'], betas=(args['b1'], args['b2']))\n",
    "optimizer_D = torch.optim.Adam(D.parameters(), lr=args['lr'], betas=(args['b1'], args['b2']))\n",
    "optimizer_info = torch.optim.Adam(\n",
    "    itertools.chain(P.parameters(),G.parameters(), D.parameters()), lr=args['lr'], betas=(args['b1'], args['b2'])\n",
    ")\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if (device == 'cuda') else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if (device == 'cuda') else torch.LongTensor\n",
    "\n",
    "def sample_image(n_row, batches_done):\n",
    "    \"\"\"noise에 의한 이미지 변화만 볼거라서 code는 0으로 다 고정\"\"\"\n",
    "    # 10*10 짜리 grid 만들려고 batch_size를 100으로 하는거임 => 10개 클래스 이미지를 10등분으로 interpolation\n",
    "    z = Variable(FloatTensor(np.random.normal(0, 1, (n_row ** 2, args['latent_dim']))))\n",
    "    static_label = to_discrete(np.array([num for _ in range(args.n_classes) for num in range(args['n_classes'])]), \n",
    "                               args['n_classes']) \n",
    "    static_code = Variable(FloatTensor(np.zeros((args['n_classes']**2, args['code_dim']))))\n",
    "    static_sample = G(z, static_label, static_code)\n",
    "    save_image(static_sample.data, \"images/static/%d.png\" % batches_done, nrow=n_row, normalize=True)\n",
    "\n",
    "    \"\"\"code의 interpolation에 의한 이미지 변화만 볼거라서 noise도 0으로 다 고정\"\"\"\n",
    "    static_z = Variable(FloatTensor(np.zeros((args['n_classes']**2, args['latent_dim']))))\n",
    "    # code 하나 고정용 벡터 생성\n",
    "    zeros = np.zeros((n_row ** 2, 1))\n",
    "    # n_row(10)번만큼 반복되는 interpolation 생성 ex) 2번 반복 [[-1],[0],[1]] => [[-1],[-1],[0],[0],[1],[1]]\n",
    "    c_varied = np.repeat(np.linspace(-1, 1, n_row)[:, np.newaxis], n_row, 0)\n",
    "    # code 하나는 0으로 고정하고 나머지 code 하나를 -1에서 1로 interpolation한 tensor 생성 (100,2)\n",
    "    c1 = Variable(FloatTensor(np.concatenate((c_varied, zeros), -1)))\n",
    "    c2 = Variable(FloatTensor(np.concatenate((zeros, c_varied), -1)))\n",
    "    # 0~9까지 class에서 하나씩만 뽑아(static_label) interpolation한 code로 image 생성\n",
    "    sample1 = G(static_z, static_label, c1)\n",
    "    sample2 = G(static_z, static_label, c2)\n",
    "    save_image(sample1.data, \"images/varying_c1/%d.png\" % batches_done, nrow=n_row, normalize=True)\n",
    "    save_image(sample2.data, \"images/varying_c2/%d.png\" % batches_done, nrow=n_row, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 2])"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, (imgs,labels) in enumerate(train_loader):\n",
    "    batch_size = imgs.shape[0]\n",
    "    \n",
    "    real = Variable(FloatTensor(batch_size, 1).fill_(1.0), requires_grad=False)\n",
    "    fake = Variable(FloatTensor(batch_size, 1).fill_(0.0), requires_grad=False)\n",
    "    real_imgs = Variable(imgs.type(FloatTensor))\n",
    "    labels = to_discrete(labels.numpy(), args['n_classes'])\n",
    "    break\n",
    "\n",
    "sampled_labels = np.random.randint(0,args['n_classes'], batch_size)\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training starts!\n",
      "[Epoch 0/200] [P loss: 0.0468] [D loss: 0.0384] [G loss: 37.3640] [info disc: 0.0000] [info cont: 0.0219] [time: 8.0]\n",
      "[Epoch 1/200] [P loss: 0.0322] [D loss: 0.0190] [G loss: 17.7814] [info disc: 0.0000] [info cont: 0.0108] [time: 16.6]\n",
      "[Epoch 2/200] [P loss: 0.0235] [D loss: 0.0138] [G loss: 15.8400] [info disc: 0.0000] [info cont: 0.0168] [time: 24.6]\n",
      "[Epoch 3/200] [P loss: 0.0014] [D loss: 0.0327] [G loss: 11.9513] [info disc: 0.0000] [info cont: 0.0078] [time: 32.5]\n",
      "[Epoch 4/200] [P loss: 0.0025] [D loss: 0.0882] [G loss: 10.7631] [info disc: 0.0000] [info cont: 0.0061] [time: 40.3]\n",
      "[Epoch 5/200] [P loss: 0.0561] [D loss: 0.2333] [G loss: 11.8809] [info disc: 0.0000] [info cont: 0.0238] [time: 48.0]\n",
      "[Epoch 6/200] [P loss: 0.0016] [D loss: 0.2601] [G loss: 12.3405] [info disc: 0.0000] [info cont: 0.0061] [time: 55.5]\n",
      "[Epoch 7/200] [P loss: 0.0022] [D loss: 0.0167] [G loss: 11.6003] [info disc: 0.0000] [info cont: 0.0066] [time: 62.3]\n",
      "[Epoch 8/200] [P loss: 0.0011] [D loss: 0.0278] [G loss: 9.8540] [info disc: 0.0000] [info cont: 0.0036] [time: 69.8]\n",
      "[Epoch 9/200] [P loss: 0.0109] [D loss: 0.0129] [G loss: 12.7350] [info disc: 0.0000] [info cont: 0.0089] [time: 78.0]\n",
      "[Epoch 10/200] [P loss: 0.0004] [D loss: 0.0432] [G loss: 10.7941] [info disc: 0.0000] [info cont: 0.0030] [time: 86.5]\n",
      "[Epoch 11/200] [P loss: 0.0250] [D loss: 0.0540] [G loss: 11.7019] [info disc: 0.0000] [info cont: 0.0135] [time: 93.6]\n",
      "[Epoch 12/200] [P loss: 0.0143] [D loss: 0.0298] [G loss: 11.3956] [info disc: 0.0000] [info cont: 0.0301] [time: 101.1]\n",
      "[Epoch 13/200] [P loss: 0.0034] [D loss: 0.0479] [G loss: 14.0374] [info disc: 0.0000] [info cont: 0.0043] [time: 109.2]\n",
      "[Epoch 14/200] [P loss: 0.0003] [D loss: 0.1442] [G loss: 13.5250] [info disc: 0.0000] [info cont: 0.0029] [time: 117.3]\n",
      "[Epoch 15/200] [P loss: 0.0009] [D loss: 0.0186] [G loss: 10.9554] [info disc: 0.0000] [info cont: 0.0028] [time: 124.8]\n",
      "[Epoch 16/200] [P loss: 0.0165] [D loss: 0.0108] [G loss: 11.6575] [info disc: 0.0000] [info cont: 0.0249] [time: 132.5]\n",
      "[Epoch 17/200] [P loss: 0.0002] [D loss: 0.0427] [G loss: 12.6485] [info disc: 0.0000] [info cont: 0.0041] [time: 140.8]\n",
      "[Epoch 18/200] [P loss: 0.0006] [D loss: 0.0535] [G loss: 10.6302] [info disc: 0.0000] [info cont: 0.0061] [time: 148.6]\n",
      "[Epoch 19/200] [P loss: 0.0003] [D loss: 0.0448] [G loss: 13.2258] [info disc: 0.0000] [info cont: 0.0025] [time: 157.1]\n",
      "[Epoch 20/200] [P loss: 0.0007] [D loss: 0.0084] [G loss: 11.6158] [info disc: 0.0000] [info cont: 0.0017] [time: 165.1]\n",
      "[Epoch 21/200] [P loss: 0.0009] [D loss: 0.0221] [G loss: 10.9709] [info disc: 0.0000] [info cont: 0.0016] [time: 173.7]\n",
      "[Epoch 22/200] [P loss: 0.0017] [D loss: 0.0062] [G loss: 11.6625] [info disc: 0.0000] [info cont: 0.0042] [time: 182.2]\n",
      "[Epoch 23/200] [P loss: 0.0016] [D loss: 0.0180] [G loss: 10.9318] [info disc: 0.0000] [info cont: 0.0012] [time: 190.4]\n",
      "[Epoch 24/200] [P loss: 0.0001] [D loss: 0.0056] [G loss: 11.3847] [info disc: 0.0000] [info cont: 0.0023] [time: 198.0]\n",
      "[Epoch 25/200] [P loss: 0.0002] [D loss: 0.0189] [G loss: 12.2726] [info disc: 0.0000] [info cont: 0.0037] [time: 205.8]\n",
      "[Epoch 26/200] [P loss: 0.0001] [D loss: 0.0068] [G loss: 10.7060] [info disc: 0.0000] [info cont: 0.0026] [time: 214.3]\n",
      "[Epoch 27/200] [P loss: 0.0015] [D loss: 0.0111] [G loss: 11.7216] [info disc: 0.0000] [info cont: 0.0018] [time: 222.6]\n",
      "[Epoch 28/200] [P loss: 0.0003] [D loss: 0.0172] [G loss: 11.5833] [info disc: 0.0000] [info cont: 0.0011] [time: 230.4]\n",
      "[Epoch 29/200] [P loss: 0.0001] [D loss: 0.0059] [G loss: 12.5844] [info disc: 0.0000] [info cont: 0.0016] [time: 237.6]\n",
      "[Epoch 30/200] [P loss: 0.0001] [D loss: 0.0086] [G loss: 10.3910] [info disc: 0.0000] [info cont: 0.0010] [time: 244.8]\n",
      "[Epoch 31/200] [P loss: 0.0000] [D loss: 0.0104] [G loss: 13.4938] [info disc: 0.0000] [info cont: 0.0050] [time: 252.4]\n",
      "[Epoch 32/200] [P loss: 0.0001] [D loss: 0.0028] [G loss: 11.6366] [info disc: 0.0000] [info cont: 0.0015] [time: 260.4]\n",
      "[Epoch 33/200] [P loss: 0.0003] [D loss: 0.0810] [G loss: 11.8102] [info disc: 0.0000] [info cont: 0.0011] [time: 267.9]\n",
      "[Epoch 34/200] [P loss: -0.0000] [D loss: 0.0383] [G loss: 18.7900] [info disc: 0.0000] [info cont: 0.0158] [time: 275.7]\n",
      "[Epoch 35/200] [P loss: 0.0001] [D loss: 0.0105] [G loss: 11.7830] [info disc: 0.0000] [info cont: 0.0031] [time: 283.9]\n",
      "[Epoch 36/200] [P loss: 0.0000] [D loss: 0.0026] [G loss: 12.5079] [info disc: 0.0000] [info cont: 0.0009] [time: 291.5]\n",
      "[Epoch 37/200] [P loss: 0.0000] [D loss: 0.0049] [G loss: 12.1004] [info disc: 0.0000] [info cont: 0.0020] [time: 299.4]\n",
      "[Epoch 38/200] [P loss: 0.0002] [D loss: 0.0016] [G loss: 13.0454] [info disc: 0.0000] [info cont: 0.0011] [time: 306.3]\n",
      "[Epoch 39/200] [P loss: 0.0004] [D loss: 0.0500] [G loss: 11.7214] [info disc: 0.0000] [info cont: 0.0022] [time: 313.4]\n",
      "[Epoch 40/200] [P loss: 0.0040] [D loss: 0.0199] [G loss: 11.4387] [info disc: 0.0000] [info cont: 0.0011] [time: 321.2]\n",
      "[Epoch 41/200] [P loss: 0.0002] [D loss: 0.0096] [G loss: 10.2855] [info disc: 0.0000] [info cont: 0.0011] [time: 328.8]\n",
      "[Epoch 42/200] [P loss: 0.0001] [D loss: 0.0062] [G loss: 12.9816] [info disc: 0.0000] [info cont: 0.0007] [time: 335.9]\n",
      "[Epoch 43/200] [P loss: 0.0003] [D loss: 0.0020] [G loss: 11.1304] [info disc: 0.0000] [info cont: 0.0010] [time: 343.7]\n",
      "[Epoch 44/200] [P loss: 0.0004] [D loss: 0.0080] [G loss: 10.2516] [info disc: 0.0000] [info cont: 0.0010] [time: 351.9]\n",
      "[Epoch 45/200] [P loss: 0.0024] [D loss: 0.0028] [G loss: 11.9192] [info disc: 0.0000] [info cont: 0.0011] [time: 359.7]\n",
      "[Epoch 46/200] [P loss: 0.0001] [D loss: 0.0060] [G loss: 10.8827] [info disc: 0.0000] [info cont: 0.0017] [time: 367.5]\n",
      "[Epoch 47/200] [P loss: 0.0060] [D loss: 0.0453] [G loss: 12.9478] [info disc: 0.0000] [info cont: 0.0031] [time: 375.0]\n",
      "[Epoch 48/200] [P loss: 0.0002] [D loss: 0.0080] [G loss: 11.4085] [info disc: 0.0000] [info cont: 0.0035] [time: 383.3]\n",
      "[Epoch 49/200] [P loss: 0.0001] [D loss: 0.0102] [G loss: 11.7004] [info disc: 0.0000] [info cont: 0.0015] [time: 391.2]\n",
      "[Epoch 50/200] [P loss: 0.0004] [D loss: 0.0131] [G loss: 9.9518] [info disc: 0.0000] [info cont: 0.0026] [time: 398.9]\n",
      "[Epoch 51/200] [P loss: 0.0003] [D loss: 0.0075] [G loss: 11.9399] [info disc: 0.0000] [info cont: 0.0012] [time: 406.7]\n",
      "[Epoch 52/200] [P loss: 0.0027] [D loss: 0.0036] [G loss: 12.5333] [info disc: 0.0000] [info cont: 0.0023] [time: 414.8]\n",
      "[Epoch 53/200] [P loss: 0.0000] [D loss: 0.0188] [G loss: 12.3710] [info disc: 0.0000] [info cont: 0.0015] [time: 422.2]\n",
      "[Epoch 54/200] [P loss: 0.0000] [D loss: 0.0059] [G loss: 10.3917] [info disc: 0.0000] [info cont: 0.0018] [time: 429.4]\n",
      "[Epoch 55/200] [P loss: 0.0000] [D loss: 0.0017] [G loss: 13.2084] [info disc: 0.0000] [info cont: 0.0023] [time: 437.3]\n",
      "[Epoch 56/200] [P loss: 0.0000] [D loss: 0.0056] [G loss: 11.5241] [info disc: 0.0000] [info cont: 0.0013] [time: 445.1]\n",
      "[Epoch 57/200] [P loss: 0.0001] [D loss: 0.0019] [G loss: 12.3837] [info disc: 0.0000] [info cont: 0.0029] [time: 452.6]\n",
      "[Epoch 58/200] [P loss: 0.0002] [D loss: 0.0065] [G loss: 10.9894] [info disc: 0.0000] [info cont: 0.0012] [time: 460.4]\n",
      "[Epoch 59/200] [P loss: 0.0000] [D loss: 0.0229] [G loss: 14.6680] [info disc: 0.0000] [info cont: 0.0032] [time: 468.2]\n",
      "[Epoch 60/200] [P loss: 0.0000] [D loss: 0.0015] [G loss: 12.5753] [info disc: 0.0000] [info cont: 0.0012] [time: 476.1]\n",
      "[Epoch 61/200] [P loss: 0.0000] [D loss: 0.0172] [G loss: 13.1076] [info disc: 0.0000] [info cont: 0.0017] [time: 483.9]\n",
      "[Epoch 62/200] [P loss: 0.0000] [D loss: 0.0064] [G loss: 10.8798] [info disc: 0.0000] [info cont: 0.0014] [time: 491.7]\n",
      "[Epoch 63/200] [P loss: -0.0000] [D loss: 0.0205] [G loss: 12.2062] [info disc: 0.0000] [info cont: 0.0081] [time: 500.0]\n",
      "[Epoch 64/200] [P loss: 0.0001] [D loss: 0.0026] [G loss: 14.5669] [info disc: 0.0000] [info cont: 0.0020] [time: 508.3]\n",
      "[Epoch 65/200] [P loss: 0.0000] [D loss: 0.0063] [G loss: 11.2745] [info disc: 0.0000] [info cont: 0.0015] [time: 516.9]\n",
      "[Epoch 66/200] [P loss: 0.0000] [D loss: 0.0023] [G loss: 12.9382] [info disc: 0.0000] [info cont: 0.0013] [time: 525.3]\n",
      "[Epoch 67/200] [P loss: 0.0004] [D loss: 0.0021] [G loss: 12.1440] [info disc: 0.0000] [info cont: 0.0012] [time: 533.0]\n",
      "[Epoch 68/200] [P loss: 0.0001] [D loss: 0.0093] [G loss: 12.3437] [info disc: 0.0000] [info cont: 0.0013] [time: 540.4]\n",
      "[Epoch 69/200] [P loss: 0.0002] [D loss: 0.0030] [G loss: 12.1173] [info disc: 0.0000] [info cont: 0.0027] [time: 547.5]\n",
      "[Epoch 70/200] [P loss: 0.0002] [D loss: 0.0007] [G loss: 11.0932] [info disc: 0.0000] [info cont: 0.0016] [time: 554.8]\n",
      "[Epoch 71/200] [P loss: 0.0000] [D loss: 0.0076] [G loss: 12.5402] [info disc: 0.0000] [info cont: 0.0006] [time: 562.3]\n",
      "[Epoch 72/200] [P loss: 0.0003] [D loss: 0.0077] [G loss: 11.7262] [info disc: 0.0000] [info cont: 0.0014] [time: 570.2]\n",
      "[Epoch 73/200] [P loss: 0.0000] [D loss: 0.0409] [G loss: 13.9398] [info disc: 0.0000] [info cont: 0.0013] [time: 578.3]\n",
      "[Epoch 74/200] [P loss: 0.0000] [D loss: 0.0037] [G loss: 10.8517] [info disc: 0.0000] [info cont: 0.0014] [time: 585.9]\n",
      "[Epoch 75/200] [P loss: 0.0000] [D loss: 0.0024] [G loss: 10.3242] [info disc: 0.0000] [info cont: 0.0011] [time: 593.5]\n",
      "[Epoch 76/200] [P loss: 0.0003] [D loss: 0.0010] [G loss: 10.9221] [info disc: 0.0000] [info cont: 0.0018] [time: 600.7]\n",
      "[Epoch 77/200] [P loss: 0.0116] [D loss: 0.0045] [G loss: 14.0512] [info disc: 0.0000] [info cont: 0.0097] [time: 607.9]\n",
      "[Epoch 78/200] [P loss: 0.0000] [D loss: 0.0020] [G loss: 11.2878] [info disc: 0.0000] [info cont: 0.0020] [time: 615.9]\n",
      "[Epoch 79/200] [P loss: 0.0001] [D loss: 0.0006] [G loss: 10.3743] [info disc: 0.0000] [info cont: 0.0013] [time: 623.2]\n",
      "[Epoch 80/200] [P loss: 0.0002] [D loss: 0.0008] [G loss: 13.1789] [info disc: 0.0000] [info cont: 0.0010] [time: 631.0]\n",
      "[Epoch 81/200] [P loss: 0.0002] [D loss: 0.0011] [G loss: 11.7789] [info disc: 0.0000] [info cont: 0.0004] [time: 639.3]\n",
      "[Epoch 82/200] [P loss: 0.0001] [D loss: 0.0098] [G loss: 13.4496] [info disc: 0.0000] [info cont: 0.0007] [time: 647.7]\n",
      "[Epoch 83/200] [P loss: 0.0016] [D loss: 0.0032] [G loss: 11.1041] [info disc: 0.0000] [info cont: 0.0003] [time: 656.1]\n",
      "[Epoch 84/200] [P loss: 0.0002] [D loss: 0.0022] [G loss: 11.2862] [info disc: 0.0000] [info cont: 0.0005] [time: 663.7]\n",
      "[Epoch 85/200] [P loss: 0.0012] [D loss: 0.0026] [G loss: 11.4774] [info disc: 0.0000] [info cont: 0.0008] [time: 671.7]\n",
      "[Epoch 86/200] [P loss: 0.0005] [D loss: 0.0179] [G loss: 10.7663] [info disc: 0.0000] [info cont: 0.0011] [time: 679.6]\n",
      "[Epoch 87/200] [P loss: 0.0000] [D loss: 0.0177] [G loss: 10.7823] [info disc: 0.0000] [info cont: 0.0017] [time: 686.8]\n",
      "[Epoch 88/200] [P loss: 0.0043] [D loss: 0.0047] [G loss: 11.1483] [info disc: 0.0000] [info cont: 0.0016] [time: 694.8]\n",
      "[Epoch 89/200] [P loss: 0.0000] [D loss: 0.0044] [G loss: 11.0199] [info disc: 0.0000] [info cont: 0.0006] [time: 702.3]\n",
      "[Epoch 90/200] [P loss: 0.0004] [D loss: 0.0006] [G loss: 11.4545] [info disc: 0.0000] [info cont: 0.0006] [time: 710.8]\n",
      "[Epoch 91/200] [P loss: 0.0000] [D loss: 0.0044] [G loss: 13.1126] [info disc: 0.0000] [info cont: 0.0013] [time: 719.7]\n",
      "[Epoch 92/200] [P loss: 0.0000] [D loss: 0.0082] [G loss: 11.1230] [info disc: 0.0000] [info cont: 0.0050] [time: 728.1]\n",
      "[Epoch 93/200] [P loss: 0.1960] [D loss: 0.0595] [G loss: 33.0619] [info disc: 0.0000] [info cont: 0.2595] [time: 735.8]\n",
      "[Epoch 94/200] [P loss: 0.0000] [D loss: 0.0199] [G loss: 13.1523] [info disc: 0.0000] [info cont: 0.0023] [time: 744.8]\n",
      "[Epoch 95/200] [P loss: 0.0000] [D loss: 0.0022] [G loss: 11.8545] [info disc: 0.0000] [info cont: 0.0007] [time: 753.1]\n",
      "[Epoch 96/200] [P loss: 0.0000] [D loss: 0.0039] [G loss: 12.4810] [info disc: 0.0000] [info cont: 0.0008] [time: 760.7]\n",
      "[Epoch 97/200] [P loss: 0.0001] [D loss: 0.0010] [G loss: 12.0529] [info disc: 0.0000] [info cont: 0.0005] [time: 768.7]\n",
      "[Epoch 98/200] [P loss: 0.0006] [D loss: 0.0022] [G loss: 11.4424] [info disc: 0.0000] [info cont: 0.0011] [time: 776.7]\n",
      "[Epoch 99/200] [P loss: 0.0001] [D loss: 0.0022] [G loss: 12.0144] [info disc: 0.0000] [info cont: 0.0010] [time: 785.1]\n",
      "[Epoch 100/200] [P loss: 0.0000] [D loss: 0.0004] [G loss: 10.2398] [info disc: 0.0000] [info cont: 0.0008] [time: 793.2]\n",
      "[Epoch 101/200] [P loss: 0.0056] [D loss: 0.0003] [G loss: 10.9164] [info disc: 0.0000] [info cont: 0.0070] [time: 800.9]\n",
      "[Epoch 102/200] [P loss: 0.0000] [D loss: 0.0007] [G loss: 11.2920] [info disc: 0.0000] [info cont: 0.0006] [time: 809.4]\n",
      "[Epoch 103/200] [P loss: 0.0000] [D loss: 0.0148] [G loss: 11.6431] [info disc: 0.0000] [info cont: 0.0006] [time: 816.7]\n",
      "[Epoch 104/200] [P loss: 0.0000] [D loss: 0.0020] [G loss: 13.0366] [info disc: 0.0000] [info cont: 0.0008] [time: 825.1]\n",
      "[Epoch 105/200] [P loss: 0.0000] [D loss: 0.0025] [G loss: 12.9606] [info disc: 0.0000] [info cont: 0.0022] [time: 832.8]\n",
      "[Epoch 106/200] [P loss: 0.0000] [D loss: 0.0007] [G loss: 11.1722] [info disc: 0.0000] [info cont: 0.0006] [time: 840.3]\n",
      "[Epoch 107/200] [P loss: 0.0000] [D loss: 0.0028] [G loss: 12.7535] [info disc: 0.0000] [info cont: 0.0014] [time: 847.6]\n",
      "[Epoch 108/200] [P loss: 0.0000] [D loss: 0.0053] [G loss: 10.9198] [info disc: 0.0000] [info cont: 0.0013] [time: 855.2]\n",
      "[Epoch 109/200] [P loss: 0.0031] [D loss: 0.0034] [G loss: 10.2834] [info disc: 0.0000] [info cont: 0.0026] [time: 862.7]\n",
      "[Epoch 110/200] [P loss: 0.0002] [D loss: 0.0029] [G loss: 9.9043] [info disc: 0.0000] [info cont: 0.0003] [time: 871.2]\n",
      "[Epoch 111/200] [P loss: 0.0000] [D loss: 0.0130] [G loss: 15.7446] [info disc: 0.0000] [info cont: 0.0030] [time: 879.3]\n",
      "[Epoch 112/200] [P loss: 0.0000] [D loss: 0.0036] [G loss: 13.5216] [info disc: 0.0000] [info cont: 0.0014] [time: 887.3]\n",
      "[Epoch 113/200] [P loss: 0.0000] [D loss: 0.0012] [G loss: 12.1573] [info disc: 0.0000] [info cont: 0.0008] [time: 895.1]\n",
      "[Epoch 114/200] [P loss: 0.0000] [D loss: 0.0031] [G loss: 11.8366] [info disc: 0.0000] [info cont: 0.0006] [time: 903.4]\n",
      "[Epoch 115/200] [P loss: 0.0000] [D loss: 0.0027] [G loss: 11.3960] [info disc: 0.0000] [info cont: 0.0012] [time: 911.1]\n",
      "[Epoch 116/200] [P loss: 0.0001] [D loss: 0.0036] [G loss: 11.6090] [info disc: 0.0000] [info cont: 0.0006] [time: 918.9]\n",
      "[Epoch 117/200] [P loss: 0.0000] [D loss: 0.0086] [G loss: 13.7573] [info disc: 0.0000] [info cont: 0.0054] [time: 926.9]\n",
      "[Epoch 118/200] [P loss: 0.0003] [D loss: 0.0017] [G loss: 11.6412] [info disc: 0.0000] [info cont: 0.0005] [time: 934.1]\n",
      "[Epoch 119/200] [P loss: 0.0003] [D loss: 0.0087] [G loss: 10.6181] [info disc: 0.0000] [info cont: 0.0011] [time: 941.7]\n",
      "[Epoch 120/200] [P loss: 0.0005] [D loss: 0.0004] [G loss: 11.4075] [info disc: 0.0000] [info cont: 0.0006] [time: 949.5]\n",
      "[Epoch 121/200] [P loss: 0.0000] [D loss: 0.0063] [G loss: 11.5169] [info disc: 0.0000] [info cont: 0.0010] [time: 957.9]\n",
      "[Epoch 122/200] [P loss: 0.0000] [D loss: 0.0006] [G loss: 12.6780] [info disc: 0.0000] [info cont: 0.0011] [time: 965.9]\n",
      "[Epoch 123/200] [P loss: 0.0002] [D loss: 0.0036] [G loss: 12.1507] [info disc: 0.0000] [info cont: 0.0005] [time: 974.0]\n",
      "[Epoch 124/200] [P loss: 0.0000] [D loss: 0.0054] [G loss: 10.6569] [info disc: 0.0000] [info cont: 0.0010] [time: 982.5]\n",
      "[Epoch 125/200] [P loss: 0.0001] [D loss: 0.0025] [G loss: 14.8113] [info disc: 0.0000] [info cont: 0.0123] [time: 990.0]\n",
      "[Epoch 126/200] [P loss: 0.0000] [D loss: 0.0013] [G loss: 11.6819] [info disc: 0.0000] [info cont: 0.0004] [time: 997.6]\n",
      "[Epoch 127/200] [P loss: 0.0000] [D loss: 0.0085] [G loss: 12.1376] [info disc: 0.0000] [info cont: 0.0005] [time: 1005.0]\n",
      "[Epoch 128/200] [P loss: 0.0000] [D loss: 0.0008] [G loss: 10.4795] [info disc: 0.0000] [info cont: 0.0011] [time: 1012.6]\n",
      "[Epoch 129/200] [P loss: 0.0000] [D loss: 0.0091] [G loss: 14.3899] [info disc: 0.0000] [info cont: 0.0036] [time: 1020.3]\n",
      "[Epoch 130/200] [P loss: 0.0001] [D loss: 0.0023] [G loss: 11.8527] [info disc: 0.0000] [info cont: 0.0004] [time: 1028.0]\n",
      "[Epoch 131/200] [P loss: 0.0000] [D loss: 0.0049] [G loss: 11.1329] [info disc: 0.0000] [info cont: 0.0014] [time: 1035.9]\n",
      "[Epoch 132/200] [P loss: 0.0001] [D loss: 0.0029] [G loss: 11.6174] [info disc: 0.0000] [info cont: 0.0004] [time: 1044.3]\n",
      "[Epoch 133/200] [P loss: 0.0000] [D loss: 0.0004] [G loss: 11.9872] [info disc: 0.0000] [info cont: 0.0011] [time: 1052.2]\n",
      "[Epoch 134/200] [P loss: 0.0000] [D loss: 0.0184] [G loss: 12.8558] [info disc: 0.0000] [info cont: 0.0004] [time: 1060.1]\n",
      "[Epoch 135/200] [P loss: 0.0000] [D loss: 0.0048] [G loss: 10.7518] [info disc: 0.0000] [info cont: 0.0006] [time: 1068.2]\n",
      "[Epoch 136/200] [P loss: 0.0014] [D loss: 0.0036] [G loss: 11.1944] [info disc: 0.0000] [info cont: 0.0012] [time: 1076.6]\n",
      "[Epoch 137/200] [P loss: 0.0011] [D loss: 0.5223] [G loss: 25.6320] [info disc: 0.0000] [info cont: 0.0109] [time: 1084.6]\n",
      "[Epoch 138/200] [P loss: 0.0000] [D loss: 0.0155] [G loss: 14.7866] [info disc: 0.0000] [info cont: 0.0034] [time: 1092.6]\n",
      "[Epoch 139/200] [P loss: 0.0000] [D loss: 0.0137] [G loss: 14.2777] [info disc: 0.0000] [info cont: 0.0016] [time: 1100.5]\n",
      "[Epoch 140/200] [P loss: 0.0002] [D loss: 0.0022] [G loss: 11.7165] [info disc: 0.0000] [info cont: 0.0015] [time: 1108.3]\n",
      "[Epoch 141/200] [P loss: 0.0001] [D loss: 0.0217] [G loss: 10.9291] [info disc: 0.0000] [info cont: 0.0008] [time: 1115.7]\n",
      "[Epoch 142/200] [P loss: 0.0002] [D loss: 0.0275] [G loss: 12.8732] [info disc: 0.0000] [info cont: 0.0008] [time: 1123.6]\n",
      "[Epoch 143/200] [P loss: 0.0002] [D loss: 0.0006] [G loss: 12.8396] [info disc: 0.0000] [info cont: 0.0009] [time: 1132.3]\n",
      "[Epoch 144/200] [P loss: 0.0002] [D loss: 0.0008] [G loss: 10.7846] [info disc: 0.0000] [info cont: 0.0006] [time: 1140.4]\n",
      "[Epoch 145/200] [P loss: 0.0001] [D loss: 0.0009] [G loss: 12.1200] [info disc: 0.0000] [info cont: 0.0005] [time: 1149.3]\n",
      "[Epoch 146/200] [P loss: 0.0003] [D loss: 0.0057] [G loss: 11.2115] [info disc: 0.0000] [info cont: 0.0006] [time: 1157.6]\n",
      "[Epoch 147/200] [P loss: 0.0001] [D loss: 0.0017] [G loss: 10.7706] [info disc: 0.0000] [info cont: 0.0012] [time: 1165.7]\n",
      "[Epoch 148/200] [P loss: 0.0001] [D loss: 0.0030] [G loss: 14.3459] [info disc: 0.0000] [info cont: 0.0026] [time: 1174.2]\n",
      "[Epoch 149/200] [P loss: 0.0002] [D loss: 0.0024] [G loss: 10.4093] [info disc: 0.0000] [info cont: 0.0003] [time: 1182.6]\n",
      "[Epoch 150/200] [P loss: 0.0014] [D loss: 0.0013] [G loss: 11.6543] [info disc: 0.0000] [info cont: 0.0007] [time: 1190.7]\n",
      "[Epoch 151/200] [P loss: 0.0002] [D loss: 0.0007] [G loss: 11.2987] [info disc: 0.0000] [info cont: 0.0014] [time: 1198.2]\n",
      "[Epoch 152/200] [P loss: 0.0001] [D loss: 0.0026] [G loss: 12.3203] [info disc: 0.0000] [info cont: 0.0115] [time: 1205.5]\n",
      "[Epoch 153/200] [P loss: 0.0000] [D loss: 0.0023] [G loss: 12.4364] [info disc: 0.0000] [info cont: 0.0007] [time: 1214.0]\n",
      "[Epoch 154/200] [P loss: 0.0004] [D loss: 0.0014] [G loss: 11.6479] [info disc: 0.0000] [info cont: 0.0010] [time: 1222.2]\n",
      "[Epoch 155/200] [P loss: 0.0002] [D loss: 0.0014] [G loss: 11.6424] [info disc: 0.0000] [info cont: 0.0006] [time: 1230.2]\n",
      "[Epoch 156/200] [P loss: 0.0000] [D loss: 0.0026] [G loss: 11.6131] [info disc: 0.0000] [info cont: 0.0017] [time: 1239.4]\n",
      "[Epoch 157/200] [P loss: 0.0002] [D loss: 0.0010] [G loss: 12.2676] [info disc: 0.0000] [info cont: 0.0011] [time: 1247.2]\n",
      "[Epoch 158/200] [P loss: 0.0001] [D loss: 0.0241] [G loss: 11.7435] [info disc: 0.0000] [info cont: 0.0009] [time: 1254.2]\n",
      "[Epoch 159/200] [P loss: 0.0003] [D loss: 0.0006] [G loss: 11.1574] [info disc: 0.0000] [info cont: 0.0012] [time: 1262.1]\n",
      "[Epoch 160/200] [P loss: 0.0001] [D loss: 0.0011] [G loss: 13.4662] [info disc: 0.0000] [info cont: 0.0024] [time: 1270.5]\n",
      "[Epoch 161/200] [P loss: 0.0001] [D loss: 0.0027] [G loss: 12.1912] [info disc: 0.0000] [info cont: 0.0010] [time: 1277.8]\n",
      "[Epoch 162/200] [P loss: 0.0008] [D loss: 0.0198] [G loss: 32.0077] [info disc: 0.0000] [info cont: 0.1961] [time: 1285.1]\n",
      "[Epoch 163/200] [P loss: 0.0002] [D loss: 0.0039] [G loss: 13.9725] [info disc: 0.0000] [info cont: 0.0016] [time: 1294.2]\n",
      "[Epoch 164/200] [P loss: 0.0007] [D loss: 0.0003] [G loss: 13.5756] [info disc: 0.0000] [info cont: 0.0004] [time: 1302.8]\n",
      "[Epoch 165/200] [P loss: 0.0004] [D loss: 0.0019] [G loss: 10.1616] [info disc: 0.0000] [info cont: 0.0006] [time: 1310.7]\n",
      "[Epoch 166/200] [P loss: 0.0006] [D loss: 0.0006] [G loss: 11.9177] [info disc: 0.0000] [info cont: 0.0006] [time: 1318.6]\n",
      "[Epoch 167/200] [P loss: 0.0001] [D loss: 0.0005] [G loss: 12.4158] [info disc: 0.0000] [info cont: 0.0008] [time: 1326.7]\n",
      "[Epoch 168/200] [P loss: 0.0008] [D loss: 0.0014] [G loss: 11.3465] [info disc: 0.0000] [info cont: 0.0005] [time: 1334.5]\n",
      "[Epoch 169/200] [P loss: 0.0001] [D loss: 0.0039] [G loss: 11.3880] [info disc: 0.0000] [info cont: 0.0005] [time: 1343.0]\n",
      "[Epoch 170/200] [P loss: 0.0001] [D loss: 0.0008] [G loss: 12.2614] [info disc: 0.0000] [info cont: 0.0005] [time: 1350.0]\n",
      "[Epoch 171/200] [P loss: 0.0004] [D loss: 0.0015] [G loss: 11.0567] [info disc: 0.0000] [info cont: 0.0005] [time: 1357.5]\n",
      "[Epoch 172/200] [P loss: 0.0003] [D loss: 0.0008] [G loss: 11.7176] [info disc: 0.0000] [info cont: 0.0008] [time: 1364.8]\n",
      "[Epoch 173/200] [P loss: 0.0005] [D loss: 0.0035] [G loss: 11.4528] [info disc: 0.0000] [info cont: 0.0004] [time: 1372.4]\n",
      "[Epoch 174/200] [P loss: 0.0000] [D loss: 0.0025] [G loss: 11.6444] [info disc: 0.0000] [info cont: 0.0004] [time: 1380.4]\n",
      "[Epoch 175/200] [P loss: 0.0002] [D loss: 0.0035] [G loss: 11.2556] [info disc: 0.0000] [info cont: 0.0004] [time: 1388.5]\n",
      "[Epoch 176/200] [P loss: 0.0002] [D loss: 0.0051] [G loss: 12.4021] [info disc: 0.0000] [info cont: 0.0007] [time: 1396.5]\n",
      "[Epoch 177/200] [P loss: 0.0004] [D loss: 0.0016] [G loss: 12.1982] [info disc: 0.0000] [info cont: 0.0009] [time: 1405.0]\n",
      "[Epoch 178/200] [P loss: 0.0001] [D loss: 0.0002] [G loss: 11.7614] [info disc: 0.0000] [info cont: 0.0025] [time: 1412.6]\n",
      "[Epoch 179/200] [P loss: 0.0044] [D loss: 0.0237] [G loss: 13.0584] [info disc: 0.0000] [info cont: 0.0023] [time: 1421.2]\n",
      "[Epoch 180/200] [P loss: 0.3027] [D loss: 0.0172] [G loss: 17.1284] [info disc: 0.0000] [info cont: 0.0223] [time: 1429.9]\n",
      "[Epoch 181/200] [P loss: 0.1550] [D loss: 0.0028] [G loss: 15.2104] [info disc: 0.0000] [info cont: 0.0017] [time: 1437.9]\n",
      "[Epoch 182/200] [P loss: 0.0000] [D loss: 0.0013] [G loss: 12.5217] [info disc: 0.0000] [info cont: 0.0014] [time: 1446.4]\n",
      "[Epoch 183/200] [P loss: 0.0000] [D loss: 0.0023] [G loss: 15.0919] [info disc: 0.0000] [info cont: 0.0014] [time: 1454.8]\n",
      "[Epoch 184/200] [P loss: 0.0001] [D loss: 0.0008] [G loss: 11.8916] [info disc: 0.0000] [info cont: 0.0007] [time: 1462.5]\n",
      "[Epoch 185/200] [P loss: 0.0009] [D loss: 0.0006] [G loss: 12.0330] [info disc: 0.0000] [info cont: 0.0004] [time: 1470.6]\n",
      "[Epoch 186/200] [P loss: 0.0002] [D loss: 0.0002] [G loss: 12.5347] [info disc: 0.0000] [info cont: 0.0006] [time: 1479.3]\n",
      "[Epoch 187/200] [P loss: 0.0004] [D loss: 0.0018] [G loss: 11.1996] [info disc: 0.0000] [info cont: 0.0005] [time: 1487.5]\n",
      "[Epoch 188/200] [P loss: 0.0002] [D loss: 0.0013] [G loss: 10.6093] [info disc: 0.0000] [info cont: 0.0006] [time: 1494.8]\n",
      "[Epoch 189/200] [P loss: 0.0001] [D loss: 0.0032] [G loss: 13.5668] [info disc: 0.0000] [info cont: 0.0005] [time: 1502.7]\n",
      "[Epoch 190/200] [P loss: 0.0001] [D loss: 0.0009] [G loss: 12.3541] [info disc: 0.0000] [info cont: 0.0011] [time: 1510.6]\n",
      "[Epoch 191/200] [P loss: 0.0003] [D loss: 0.0002] [G loss: 11.2624] [info disc: 0.0000] [info cont: 0.0007] [time: 1519.0]\n",
      "[Epoch 192/200] [P loss: 0.0001] [D loss: 0.0002] [G loss: 11.8051] [info disc: 0.0000] [info cont: 0.0010] [time: 1526.8]\n",
      "[Epoch 193/200] [P loss: 0.0002] [D loss: 0.0020] [G loss: 12.9897] [info disc: 0.0000] [info cont: 0.0015] [time: 1535.4]\n",
      "[Epoch 194/200] [P loss: 0.0000] [D loss: 0.0012] [G loss: 13.0984] [info disc: 0.0000] [info cont: 0.0013] [time: 1542.5]\n",
      "[Epoch 195/200] [P loss: 0.1228] [D loss: 0.0062] [G loss: 15.7876] [info disc: 0.0000] [info cont: 0.0305] [time: 1550.2]\n",
      "[Epoch 196/200] [P loss: 0.0005] [D loss: 0.0003] [G loss: 11.7758] [info disc: 0.0000] [info cont: 0.0004] [time: 1558.1]\n",
      "[Epoch 197/200] [P loss: 0.0001] [D loss: 0.0004] [G loss: 12.3807] [info disc: 0.0000] [info cont: 0.0008] [time: 1566.3]\n",
      "[Epoch 198/200] [P loss: 0.0002] [D loss: 0.0099] [G loss: 11.0676] [info disc: 0.0000] [info cont: 0.0004] [time: 1574.5]\n",
      "[Epoch 199/200] [P loss: 0.0001] [D loss: 0.0060] [G loss: 12.3598] [info disc: 0.0000] [info cont: 0.0004] [time: 1582.7]\n"
     ]
    }
   ],
   "source": [
    "start = time.time() ; print('Training starts!')\n",
    "\n",
    "for epoch in range(args['Epochs']):\n",
    "    for i, (imgs,labels) in enumerate(train_loader):\n",
    "        batch_size = imgs.shape[0]\n",
    "        real = Variable(FloatTensor(batch_size, 1).fill_(1.0), requires_grad=False).to(device)\n",
    "        fake = Variable(FloatTensor(batch_size, 1).fill_(0.0), requires_grad=False).to(device)\n",
    "        \n",
    "        real_imgs = Variable(imgs.type(FloatTensor)).to(device)\n",
    "        labels = to_discrete(labels.numpy(), args['n_classes']).to(device)\n",
    "        \n",
    "        encoded = E(real_imgs)\n",
    "        \n",
    "        \n",
    "        # -----------------\n",
    "        #  Train Predictor\n",
    "        # -----------------\n",
    "        optimizer_P.zero_grad()\n",
    "        \n",
    "        predicted, _, _, _, _ = P(encoded)\n",
    "        loss_P = predict_loss(predicted, labels)\n",
    "\n",
    "        loss_P.backward(retain_graph=True)\n",
    "        optimizer_P.step()\n",
    "        \n",
    "        # -----------------\n",
    "        #  Train Generator (input = latent, label, code)\n",
    "        # -----------------\n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        \n",
    "        # z = Variable(FloatTensor(np.random.normal(0, 1, (batch_size, args['latent_dim']))))\n",
    "        # label_input = to_discrete(np.random.randint(0, args['n_classes'], batch_size), args['n_classes'])\n",
    "        # code_input = Variable(FloatTensor(np.random.uniform(-1,1, (batch_size, args['cont_dim']))))\n",
    "        _, cont_code_P, cont_code_G, disc_code, latent = P(encoded)\n",
    "        fake_imgs = G(cont_code_P, cont_code_G, disc_code, latent)\n",
    "        reality, _, _ = D(fake_imgs)\n",
    "        \n",
    "        loss_adv = adversarial_loss(reality, real) # fake_imgs의 분류(D) 결과가 최대한 1(real)로 분류되도록 G 학습\n",
    "        loss_recon = recon_loss(real_imgs, fake_imgs)\n",
    "        loss_G = loss_adv + 100*loss_recon\n",
    "        loss_G.backward(retain_graph=True)\n",
    "        optimizer_G.step()\n",
    "    \n",
    "        # -----------------\n",
    "        #  Train Discriminator\n",
    "        # -----------------\n",
    "        optimizer_D.zero_grad()\n",
    "        \n",
    "        # real or fake pred score\n",
    "        pred_real, _, _ = D(real_imgs)\n",
    "        pred_fake, _, _ = D(fake_imgs.detach())\n",
    "        loss_D_real = adversarial_loss(pred_real, real) # real_imgs는 D가 1(real)로 분류하도록 D 학습 \n",
    "        loss_D_fake = adversarial_loss(pred_fake, fake) # fake_imgs는 D가 0(fake)로 분류하도록 D 학습\n",
    "        loss_D = (loss_D_real + loss_D_fake) / 2\n",
    "        \n",
    "        loss_D.backward(retain_graph=True)\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # ------------------\n",
    "        # Information Loss\n",
    "        # ------------------\n",
    "        optimizer_info.zero_grad()\n",
    "        \n",
    "        # sampled_labels = labels\n",
    "        \n",
    "        # gt_labels = Variable(disc_code, requires_grad=False).to(device)\n",
    "        \n",
    "        # z = Variable(FloatTensor(np.random.normal(0, 1, (batch_size, args['latent_dim']))))\n",
    "        \n",
    "        label_input = to_discrete(sampled_labels, args['n_classes'])\n",
    "        code_input = Variable(FloatTensor(np.random.uniform(-1, 1, (batch_size, args['cont_dim']))))\n",
    "        \n",
    "        fake_imgs = G(cont_code_P, cont_code_G, disc_code, latent)\n",
    "        _, pred_label, pred_code = D(fake_imgs) # D라고 해놨지만 Q head의 출력임\n",
    "        cont_code = torch.cat((cont_code_P, cont_code_G), dim=-1)\n",
    "        # loss_info_disc = lambda_disc * discrete_loss(pred_label, gt_labels) # 실제 레이블 예측(이산 CELoss)\n",
    "        loss_info_cont = lambda_cont * continuous_loss(pred_code, cont_code)# code input 예측(연속 MSELoss)\n",
    "        loss_info =  loss_info_cont\n",
    "                    \n",
    "        loss_info.backward()\n",
    "        optimizer_info.step()\n",
    "        \n",
    "        # code만 고정된 경우 생성 image와 code만 interpolation하는 경우 생성 image 저장\n",
    "        batches_done = epoch * len(train_loader) + i\n",
    "        # if batches_done % args['sample_interval'] == 0:\n",
    "        #     sample_image(n_row=10, batches_done=batches_done)\n",
    "            \n",
    "    # --------------\n",
    "    # Log Progress\n",
    "    # --------------\n",
    "    print(\n",
    "        \"[Epoch %d/%d] [P loss: %.4f] [D loss: %.4f] [G loss: %.4f] [info disc: %.4f] [info cont: %.4f] [time: %.1f]\"\n",
    "        % (epoch, args['Epochs'], \n",
    "        #    i, len(train_loader),\n",
    "           loss_P.item(),\n",
    "           loss_D.item(), \n",
    "           loss_G.item(), \n",
    "        #    loss_info_disc.item(),\n",
    "            0,\n",
    "           loss_info_cont.item(),\n",
    "           time.time()-start)\n",
    "    )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0367, 0.0079], device='cuda:2', grad_fn=<SelectBackward0>)\n",
      "tensor([-153.2464,  -73.7961], device='cuda:2')\n",
      "tensor([-0.5931, -0.0178,  0.0176], device='cuda:2', grad_fn=<SelectBackward0>)\n",
      "tensor([-0.6220, -0.0603,  0.0923], device='cuda:2', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(pred_label[0])\n",
    "print(gt_labels[0])\n",
    "\n",
    "print(pred_code[0])\n",
    "print(cont_code[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iamge, label in test_loader:\n",
    "    image = image.to(device)\n",
    "    encoded = E(image)\n",
    "    predicted, cont_code_P, cont_code_G, disc_code, latent = P(encoded)\n",
    "    fake_imgs = G(cont_code_P, cont_code_G, disc_code, latent)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb06974c700>"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x1440 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "IDX = 16\n",
    "\n",
    "plt.figure(figsize=(10,20))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(image[IDX].permute(1,2,0).cpu().detach().numpy())\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(fake_imgs[IDX].permute(1,2,0).cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted,_,_,_,_ = P(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted, cont_code_P, cont_code_G, disc_code, latent = P(encoded)\n",
    "fake = G(cont_code_P, cont_code_G, disc_code, latent)\n",
    "\n",
    "save_image(fake,\"apapap.png\", nrow=10, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_row = 10\n",
    "zeros = np.zeros((n_row ** 2, 1))\n",
    "cont_P_varied = np.repeat(np.linspace(-2, 2, n_row)[:, np.newaxis], n_row, 0)\n",
    "cont_G_varied = np.repeat(np.linspace(-1, 1, n_row)[:, np.newaxis], n_row, 0)\n",
    "c1 = Variable(FloatTensor(np.concatenate((cont_G_varied, zeros), -1))).to(device)\n",
    "fake_p = G(FloatTensor(cont_P_varied).to(device), cont_code_G[:100], disc_code[:100], latent[:100])\n",
    "fake_g = G(cont_code_P[:100], c1.to(device), disc_code[:100], latent[:100])\n",
    "save_image(fake_g,\"varied_c2.png\", nrow=10, normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.7153, device='cuda:2', grad_fn=<MinBackward1>) tensor(0.3685, device='cuda:2', grad_fn=<MaxBackward1>)\n",
      "tensor(-2.3641, device='cuda:2', grad_fn=<MinBackward1>) tensor(2.5974, device='cuda:2', grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(torch.min(cont_code_G), torch.max(cont_code_G))\n",
    "print(torch.min(cont_code_P), torch.max(cont_code_P))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.5974, device='cuda:2', grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(cont_code_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_image(n_row, batches_done):\n",
    "    \"\"\"noise에 의한 이미지 변화만 볼거라서 code는 0으로 다 고정\"\"\"\n",
    "    # 10*10 짜리 grid 만들려고 batch_size를 100으로 하는거임 => 10개 클래스 이미지를 10등분으로 interpolation\n",
    "    z = Variable(FloatTensor(np.random.normal(0, 1, (n_row ** 2, args['latent_dim']))))\n",
    "    static_label = to_discrete(np.array([num for _ in range(args.n_classes) for num in range(args['n_classes'])]), \n",
    "                               args['n_classes']) \n",
    "    static_code = Variable(FloatTensor(np.zeros((args['n_classes']**2, args['code_dim']))))\n",
    "    static_sample = G(z, static_label, static_code)\n",
    "    save_image(static_sample.data, \"images/static/%d.png\" % batches_done, nrow=n_row, normalize=True)\n",
    "\n",
    "    \"\"\"code의 interpolation에 의한 이미지 변화만 볼거라서 noise도 0으로 다 고정\"\"\"\n",
    "    static_z = Variable(FloatTensor(np.zeros((args['n_classes']**2, args['latent_dim']))))\n",
    "    # code 하나 고정용 벡터 생성\n",
    "    zeros = np.zeros((n_row ** 2, 1))\n",
    "    # n_row(10)번만큼 반복되는 interpolation 생성 ex) 2번 반복 [[-1],[0],[1]] => [[-1],[-1],[0],[0],[1],[1]]\n",
    "    c_varied = np.repeat(np.linspace(-1, 1, n_row)[:, np.newaxis], n_row, 0)\n",
    "    # code 하나는 0으로 고정하고 나머지 code 하나를 -1에서 1로 interpolation한 tensor 생성 (100,2)\n",
    "    c1 = Variable(FloatTensor(np.concatenate((c_varied, zeros), -1)))\n",
    "    c2 = Variable(FloatTensor(np.concatenate((zeros, c_varied), -1)))\n",
    "    # 0~9까지 class에서 하나씩만 뽑아(static_label) interpolation한 code로 image 생성\n",
    "    sample1 = G(static_z, static_label, c1)\n",
    "    sample2 = G(static_z, static_label, c2)\n",
    "    save_image(sample1.data, \"images/varying_c1/%d.png\" % batches_done, nrow=n_row, normalize=True)\n",
    "    save_image(sample2.data, \"images/varying_c2/%d.png\" % batches_done, nrow=n_row, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-100.2998)"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[1,0],\n",
    "                  [1,0],\n",
    "                  [0,1]],dtype=torch.float32)\n",
    "b = torch.randn(a.shape) \n",
    "\n",
    "discrete_loss(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "997969026bb0563814df38f7ef8affc802fa04c571d985f23020a609d23c35a7"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('opcode': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
